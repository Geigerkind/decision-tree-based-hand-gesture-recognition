\subsection{Minimierung der Instruktionen einer Rückgabe}
Der Wahlklassifizierer addiert die Wahrscheinlichkeiten für jede Klasse jedes Entcheidungsbaumes. Dadurch muss der Entscheidungsbaum eine derartige Wahrscheinlichkeitsverteilung als Ergebnis zurückgeben. In
Sektion \ref{sec:cCodeTree} wurde das durch die Zuweisung zu einem Parameter gelöst.
\begin{lstlisting}[label=lst:assemblyBlattReturn,caption={Beispiel des Assemblycodes der Rückgabe der Wahrscheinlichkeitsverteilung eines Entscheidungsbaumes.}]
01: ldi r24,0
02: ldi r25,0
03: ldi r26,lo8(-128)
04: ldi r27,lo8(63)
05: st Z,r24
06: std Z+1,r25
07: std Z+2,r26
08: std Z+3,r27
09: std Z+4,__zero_reg__
10: std Z+5,__zero_reg__
11: std Z+6,__zero_reg__
12: std Z+7,__zero_reg__
13: std Z+8,__zero_reg__
14: std Z+9,__zero_reg__
15: std Z+10,__zero_reg__
16: std Z+11,__zero_reg__
17: std Z+12,__zero_reg__
18: std Z+13,__zero_reg__
19: std Z+14,__zero_reg__
20: std Z+15,__zero_reg__
21: ret
\end{lstlisting}
Listing \ref{lst:assemblyBlattReturn} zeigt das Assembly das generiert wird für die Zuweisung von 4 Klassen 1.0, 0.0, 0.0 und 0.0. Der Kompiler optimiert das bereits, indem für jedes Ergebnis ein eigener
Basic block erzeugt wird. Zusätzlich könnte die C-Code Generierung für die Fälle indem das Ergebnis 0 ist keine Zuweisung erzeugen. Wenn allerdings von dem Worst-Case Szenario ausgegangen wird
hat diese Optimierung keine Wirkung, da jeder Klasse eine Wahrscheinlichkeit zugeordnet wird.
\begin{lstlisting}[label=lst:assemblyBlattReturnDiskret,caption={Beispiel des Assemblycodes der Rückgabe eines diskreten Wahlklassifizierers.}]
01: ldi r24,lo8(1)
02: ret
\end{lstlisting}
Ein weiterer Ansatz ist den Wahlklassifizierer diskret zu modelerieren. Anstatt die Wahrscheinlichkeiten zu addieren, werden die erkannten Klassen zu gleichen Teilen gezählt und addiert. Der Rückgabeblock
verringert sich dann auf genau 2 Instruktionen (siehe Listing \ref{lst:assemblyBlattReturnDiskret}). Auch diese Rückgabe kann der Kompiler in einzelne Basic blocks extrahieren. Diese Optimierung ist hier sogar
noch effektiver, da es nur genau $N$ verschiedene Rückgabewerte gibt, für $N$ mögliche Klassen. Tests haben ergeben, dass mindestens $66\%$ Reduzierung der Programmgröße damit möglich ist.
\newline
\newline
Der Nachteil ist, dass die Ergebnisse instabil werden können, wenn viele Rückgaben nicht klar Eindeutig sind, sondern die Klasse nur eine knappe Mehrheit haben. Das ist insbesonders der Fall in
Kombination mit einem hohen Wert für \texttt{min\_samples\_leaf}. Dennoch kann auf dieser Art und Weise die Programmgröße von einem Teil des Waldes oder der ganze Wald reduziert werden und die
Stabilität des gefundenen Baumes mit den Testmengen revalidiert werden. Tests haben ergeben, dass die Erkennungsgenauigkeit der Testmengen nur geringfügig schwankt.