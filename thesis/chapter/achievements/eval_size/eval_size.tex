\section{Programmgröße}
\label{sec:eval_size}
Generell gilt, je größer und dichter der Entscheidungswald ist, desto höher ist die Erkennungsgenauigkeit. Aus diesem Grund sollte immer der vollständige Programmspeicher ausgenutzt werden. Allerdings nimmt der
Zuwachs an Erkennungsgenauigkeit mit jeder weiteren Teilung ab bei der Konstruktion eines Entscheidungsbaumes.
\newline
\newline
Scikit-Learn bietet viele Parameter, um die Teilung zu steuern. Dies mag die potentielle
Erkennungsgenauigkeit eines Baumes leicht verringern, kann dafür aber die Größe stark verringern. Dadurch jönnen tiefere Bäume verwendet werden oder mehr Bäume in einem Entscheidungswald, was potentiell
einen größeren Zuwachs der Erkennungsgenauigkeit verspricht.
\newline
\newline
Die Größe und Dichte eines Entscheidungswaldes haben einen direkten Einfluss auf die generierten Instruktionen. Je größer und dichter, desto mehr Instruktionen. Aus diesem Grund soll einerseits der
Zuwachs der Erkennungsgenauigkeit pro Vergleich maximiert werden und andererseits die Instruktionskosten pro Vergleich und Blattkosten minimiert werden.

\input{chapter/achievements/eval_size/scikit_learn}
\input{chapter/achievements/eval_size/minimizing_instructions}
\input{chapter/achievements/eval_size/minimizing_instructions_return}
